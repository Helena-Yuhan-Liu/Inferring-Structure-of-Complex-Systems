%% Setup data

% this code loads data 'digit*.mat' generated by converter.m wrtten by 
% Dr. ruslan Salakhutdinov and downloaded from
%   http://www.utstat.toronto.edu/~rsalakhu/code_DBM/

% Then, construct:
% A: n by 784
% x: 784 by 10
% y: n by 10

format compact; 
clc; clear;

% Initialize Ax=y
np=784; 
ntrain=60000; ntest=10000; 
ytrain=NaN(ntrain,10); 
ytest=NaN(ntest,10); 
Atrain=NaN(ntrain,np); % rank(Atrain) = 712
Atest=NaN(ntest,np); % rank(Atest) = 661

% iterate through the 9 digits and extract training and test data
itrain=0; itest=0; 
for ii=0:9      
    yi=zeros(1,10);     % each column in label matrix y
    if ii==0
        yi(10)=1; idx=10;
    else
        yi(ii)=1; idx=ii; 
    end 
    
    % load data
    load(['./digitData/digit' num2str(ii) '.mat']); % training data
    itrain = itrain+size(D,1); 
    Atrain(itrain-size(D,1)+1:itrain,:) = D; % add data to A
    ytrain(itrain-size(D,1)+1:itrain,:) = repmat(yi,[size(D,1) 1]);
    
    load(['./digitData/test' num2str(ii) '.mat']); % repeat for test
    itest = itest+size(D,1); 
    Atest(itest-size(D,1)+1:itest,:) = D;
    ytest(itest-size(D,1)+1:itest,:) = repmat(yi,[size(D,1) 1]); 
end 

disp('Done loading data'); 

% % visualize an example image
% figure; image(reshape(D(123,:),[28, 28])'); 

%% Different Regression
% Judging from the dimension this is an overdetermined system, since 
% the number of equation n far exceeds the unknowns available

% several different lasso() are tried
% Alpha controls how much sparsity versus small 2norm should be promoted 
% Alpha=1 corresponds to lasso, alpha=0 is similar to ridge, 
% anything in between is elastic net 
% The objective function is can be found at 
%   https://www.mathworks.com/help/stats/lasso-and-elastic-net.html

% Ridge (Tikhonov regularization) punishes L2 norm

disp('begin regression'); 

x1 = pinv(Atrain)*ytrain;       % pseudoinverse

x2 = Atrain\ytrain;     % QR-based backslash
 
x3=NaN(np,10);      % Lasso with alpha=1
for kk=1:10
    x3(:, kk) = lasso(Atrain,ytrain(:,kk),'Lambda',0.1); % increasing lambda was worse off 
end

x4=NaN(np,10);      % Lasso with alpha=0.5
for kk=1:10
    x4(:, kk) = lasso(Atrain,ytrain(:,kk),'Lambda',0.1,'Alpha', 0.5);
end

x5=NaN(np,10);      % Lasso with alpha=0.1
for kk=1:10
    x5(:, kk) = lasso(Atrain,ytrain(:,kk),'Lambda',0.1,'Alpha', 0.1);
end

x6=NaN(np,10);      % Ridge (L2) regularization
for kk=1:10
    x6(:, kk) = ridge(ytrain(:,kk), Atrain(:,2:end), 0.5, 0);
end


%% Save data

fname1='x_all.mat'; 
save(fname1, 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', ...
    'Atrain', 'Atest', 'ytrain', 'ytest','ntest','ntrain'); 

%% Rank pixel contribution and test

npix_list = 25:25:780; 
alg_list = 1:6; 
alg_name = {'pinv', '\', 'lasso, \alpha=1',...
     'lasso, \alpha=0.5', 'lasso, \alpha=0.1', 'ridge'}; 
c_list = {'c', 'm', 'r', 'g', 'b', 'k', 'y'};
 
% Get error rate vs number of top pixels used
ErrRate = zeros(length(alg_list),length(npix_list)); 
Err2 = ErrRate; 
for ii = 1:length(alg_list) 
    for jj = 1:length(npix_list)
        npix = npix_list(jj); algn = alg_list(ii); 
        eval(['x = x' num2str(algn) ';']); 
        xE = max(abs(x), [], 2); % Tried 1-norm, 2-norm, but turns out inf-norm works better in general
        [~, ixE] = sort( xE, 'descend' );
        x_red=zeros(size(x)); x_red(ixE(1:npix),:)=x(ixE(1:npix),:); 
        
        % Look at test error only, no one cares about training error 
        [~, labPred] = max(Atest*x_red,[],2); [~, labTrue] = max(ytest,[],2);
        ErrRate(ii,jj) = sum(labPred~=labTrue)/ntest;
        Err2(ii,jj) = norm(ytest-Atest*x_red)/norm(ytest);
    end 
end 

% Error rate plot
% seems like pinv and \ achieves better results with large n, but lasso has
% lower pareto optimal
figure; 
for ii=alg_list
    if ii==4 || ii==5 || ii==1
            mstyle = '+';
        else 
            mstyle = '--';
        end 
    plot(npix_list, ErrRate(ii,:), [c_list{ii} mstyle]); 
    if ii==1; hold on; end 
end 
hold off; xlabel('Number of Pixels used'); ylabel('Error rate'); 
legend(alg_name); 
set(gcf, 'position', [100 100 450 350]); set(gcf,'color','w');

% visualize the most important pixel contribution
figure; 
subplot(2,2,1); pcolor(reshape(max(abs(x2), [], 2)~=0,[28, 28])'); 
title('Nonzero Pixel, \');  
xlabel('row pixels'); ylabel('column pixels'); 
subplot(2,2,2); pcolor(reshape(max(abs(x3)~=0, [], 2),[28, 28])'); 
title('Nonzero Pixel, lasso \alpha=1'); 
xlabel('row pixels'); ylabel('column pixels'); 
subplot(2,2,3); pcolor(reshape(max(abs(x4)~=0, [], 2),[28, 28])'); 
title('Nonzero Pixel, lasso \alpha=0.5'); 
xlabel('row pixels'); ylabel('column pixels'); 
subplot(2,2,4); pcolor(reshape(max(abs(x5)~=0, [], 2),[28, 28])'); 
title('Nonzero Pixel, lasso \alpha=0.1'); 
xlabel('row pixels'); ylabel('column pixels'); 
set(gcf, 'position', [100 100 650 550]); set(gcf,'color','w');

%% Test for each individual digit 

% Don't have to redo the training, the b now becomes ith column of b
% 1 meaning it is the digit of interest, 0 is not 
% This becomes a binary classification problem 
% Just need to analyze each column individually 

npix_list = 25:25:780; 
alg_list = 1:6; 
alg_name = {'pinv', '\', 'lasso, \alpha=1',...
     'lasso, \alpha=0.5', 'lasso, \alpha=0.1', 'ridge'}; 

% Loop through all 10 digits and compute the error rate 
for dig=1:10 
    
    % Get error rate vs number of top pixels used
    ErrRate = zeros(length(alg_list),length(npix_list)); 
    for ii = 1:length(alg_list) 
        for jj = 1:length(npix_list)
            npix = npix_list(jj); algn = alg_list(ii); 
            eval(['x = x' num2str(algn) '(:,dig);']);
%             eval(['x = x' num2str(algn) '{dig};']); 
            xE = abs(x); [~, ixE] = sort( xE, 'descend' );
            x_red=zeros(size(x)); x_red(ixE(1:npix))=x(ixE(1:npix)); 

            % Look at test error only, no one cares about training error 
            % y greater than 0.5 will be set to yes 
            labPred = (Atest*x_red > 0.5); 
            labTrue = ytest(:,dig); 
            ErrRate(ii,jj) = sum(labPred~=labTrue)/ntest;
        end 
    end 

    % Error rate plot for each digit 
    figure; 
    for ii=alg_list
        if ii==4 || ii==5 || ii==1
            mstyle = '+';
        else 
            mstyle = '--';
        end 
        plot(npix_list, ErrRate(ii,:), [c_list{ii} mstyle]); 
        if ii==1; hold on; end 
    end 
    hold off; xlabel('Number of Pixels used'); ylabel('Error rate'); 
    ylim([0 0.12]); legend(alg_name);  
    set(gcf, 'position', [100 100 450 350]); set(gcf,'color','w');
end

% Sparsity plot for digit 6
figure; 
subplot(2,2,1); pcolor(reshape(max(abs(x2(:,6)), [], 2)~=0,[28, 28])'); 
title('Nonzero Pixel, \');  
xlabel('row pixels'); ylabel('column pixels'); 
subplot(2,2,2); pcolor(reshape(max(abs(x3(:,6))~=0, [], 2),[28, 28])'); 
title('Nonzero Pixel, lasso \alpha=1'); 
xlabel('row pixels'); ylabel('column pixels'); 
subplot(2,2,3); pcolor(reshape(max(abs(x4(:,6))~=0, [], 2),[28, 28])'); 
title('Nonzero Pixel, lasso \alpha=0.5'); 
xlabel('row pixels'); ylabel('column pixels'); 
subplot(2,2,4); pcolor(reshape(max(abs(x5(:,6))~=0, [], 2),[28, 28])'); 
title('Nonzero Pixel, lasso \alpha=0.1'); 
xlabel('row pixels'); ylabel('column pixels'); 
set(gcf, 'position', [100 100 650 550]); set(gcf,'color','w');
